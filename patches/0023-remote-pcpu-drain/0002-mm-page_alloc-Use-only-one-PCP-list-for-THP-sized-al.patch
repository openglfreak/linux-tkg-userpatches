From patchwork Fri Jun 24 12:54:18 2022
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
X-Patchwork-Submitter: Mel Gorman <mgorman@techsingularity.net>
X-Patchwork-Id: 12894468
Return-Path: <owner-linux-mm@kvack.org>
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
	aws-us-west-2-korg-lkml-1.web.codeaurora.org
Received: from kanga.kvack.org (kanga.kvack.org [205.233.56.17])
	by smtp.lore.kernel.org (Postfix) with ESMTP id D6744C43334
	for <linux-mm@archiver.kernel.org>; Fri, 24 Jun 2022 12:54:58 +0000 (UTC)
Received: by kanga.kvack.org (Postfix)
	id 71BD58E0218; Fri, 24 Jun 2022 08:54:58 -0400 (EDT)
Received: by kanga.kvack.org (Postfix, from userid 40)
	id 6CAC68E020E; Fri, 24 Jun 2022 08:54:58 -0400 (EDT)
X-Delivered-To: int-list-linux-mm@kvack.org
Received: by kanga.kvack.org (Postfix, from userid 63042)
	id 5BA818E0218; Fri, 24 Jun 2022 08:54:58 -0400 (EDT)
X-Delivered-To: linux-mm@kvack.org
Received: from relay.hostedemail.com (smtprelay0017.hostedemail.com
 [216.40.44.17])
	by kanga.kvack.org (Postfix) with ESMTP id 4D57E8E020E
	for <linux-mm@kvack.org>; Fri, 24 Jun 2022 08:54:58 -0400 (EDT)
Received: from smtpin26.hostedemail.com (a10.router.float.18 [10.200.18.1])
	by unirelay09.hostedemail.com (Postfix) with ESMTP id 1F04734F00
	for <linux-mm@kvack.org>; Fri, 24 Jun 2022 12:54:58 +0000 (UTC)
X-FDA: 79613124276.26.7476A5F
Received: from outbound-smtp55.blacknight.com (outbound-smtp55.blacknight.com
 [46.22.136.239])
	by imf13.hostedemail.com (Postfix) with ESMTP id 628C820017
	for <linux-mm@kvack.org>; Fri, 24 Jun 2022 12:54:57 +0000 (UTC)
Received: from mail.blacknight.com (pemlinmail06.blacknight.ie
 [81.17.255.152])
	by outbound-smtp55.blacknight.com (Postfix) with ESMTPS id 09654FAC58
	for <linux-mm@kvack.org>; Fri, 24 Jun 2022 13:54:56 +0100 (IST)
Received: (qmail 7478 invoked from network); 24 Jun 2022 12:54:55 -0000
Received: from unknown (HELO morpheus.112glenside.lan)
 (mgorman@techsingularity.net@[84.203.198.246])
  by 81.17.254.9 with ESMTPA; 24 Jun 2022 12:54:55 -0000
From: Mel Gorman <mgorman@techsingularity.net>
To: Andrew Morton <akpm@linux-foundation.org>
Cc: Nicolas Saenz Julienne <nsaenzju@redhat.com>,
	Marcelo Tosatti <mtosatti@redhat.com>,
	Vlastimil Babka <vbabka@suse.cz>,
	Michal Hocko <mhocko@kernel.org>,
	Hugh Dickins <hughd@google.com>,
	Yu Zhao <yuzhao@google.com>,
	Marek Szyprowski <m.szyprowski@samsung.com>,
	LKML <linux-kernel@vger.kernel.org>,
	Linux-MM <linux-mm@kvack.org>,
	Mel Gorman <mgorman@techsingularity.net>
Subject: [PATCH 2/7] mm/page_alloc: Use only one PCP list for THP-sized
 allocations
Date: Fri, 24 Jun 2022 13:54:18 +0100
Message-Id: <20220624125423.6126-3-mgorman@techsingularity.net>
X-Mailer: git-send-email 2.35.3
In-Reply-To: <20220624125423.6126-1-mgorman@techsingularity.net>
References: <20220624125423.6126-1-mgorman@techsingularity.net>
MIME-Version: 1.0
ARC-Authentication-Results: i=1;
	imf13.hostedemail.com;
	dkim=none;
	dmarc=none;
	spf=pass (imf13.hostedemail.com: domain of mgorman@techsingularity.net
 designates 46.22.136.239 as permitted sender)
 smtp.mailfrom=mgorman@techsingularity.net
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed;
 d=hostedemail.com;
	s=arc-20220608; t=1656075297;
	h=from:from:sender:reply-to:subject:subject:date:date:
	 message-id:message-id:to:to:cc:cc:mime-version:mime-version:
	 content-type:content-transfer-encoding:content-transfer-encoding:
	 in-reply-to:in-reply-to:references:references;
	bh=XgscNebn/w3HJNudLhCXREo1bw93QiM4OgHsfyXOsQs=;
	b=2XaFf/DHbh88CMMiBaPDv/6Bg78AkSgg1eVxrnW07VxdJL67VoELtc21ZnAdfoe5mRSBqk
	ms1GWF9EYYXRBwHSoQLg8qdk9GzYsrHhNZD0BY0A7LHy8W3iRMXFbhEJTeJ46SFDP5j7Wy
	2AInUJ0/OX9u1SA6WSIGUWIAJcQz7bQ=
ARC-Seal: i=1; s=arc-20220608; d=hostedemail.com; t=1656075297; a=rsa-sha256;
	cv=none;
	b=aEdq4E9IrKr1RdXx1lEUo9vkan1L/nJNZO4XwyGbmHQkX5yoecjJPkHa73x3sOxk13/dJ6
	dNoZktPixSziGLodW/kbaMR1QZWjXS838cTHlPu2W1TEqs+yWsnKXSRzAUYI9eak5QYZ7q
	/rPKd5JNRm7085i6EEKRuwYbPtK6VnA=
X-Stat-Signature: no73totwgo1hhtfja7adbure1p8fw36j
X-Rspamd-Queue-Id: 628C820017
X-Rspam-User: 
Authentication-Results: imf13.hostedemail.com;
	dkim=none;
	dmarc=none;
	spf=pass (imf13.hostedemail.com: domain of mgorman@techsingularity.net
 designates 46.22.136.239 as permitted sender)
 smtp.mailfrom=mgorman@techsingularity.net
X-Rspamd-Server: rspam12
X-HE-Tag: 1656075297-383120
X-Bogosity: Ham, tests=bogofilter, spamicity=0.000000, version=1.2.4
Sender: owner-linux-mm@kvack.org
Precedence: bulk
X-Loop: owner-majordomo@kvack.org
List-ID: <linux-mm.kvack.org>

The per_cpu_pages is cache-aligned on a standard x86-64 distribution
configuration but a later patch will add a new field which would push the
structure into the next cache line.  Use only one list to store THP-sized
pages on the per-cpu list.  This assumes that the vast majority of
THP-sized allocations are GFP_MOVABLE but even if it was another type, it
would not contribute to serious fragmentation that potentially causes a
later THP allocation failure.  Align per_cpu_pages on the cacheline
boundary to ensure there is no false cache sharing.

After this patch, the structure sizing is;

struct per_cpu_pages {
        int                        count;                /*     0     4 */
        int                        high;                 /*     4     4 */
        int                        batch;                /*     8     4 */
        short int                  free_factor;          /*    12     2 */
        short int                  expire;               /*    14     2 */
        struct list_head           lists[13];            /*    16   208 */

        /* size: 256, cachelines: 4, members: 6 */
        /* padding: 32 */
} __attribute__((__aligned__(64)));

Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
Tested-by: Minchan Kim <minchan@kernel.org>
Acked-by: Minchan Kim <minchan@kernel.org>
Acked-by: Vlastimil Babka <vbabka@suse.cz>
---
 include/linux/mmzone.h | 11 +++++++----
 mm/page_alloc.c        |  4 ++--
 2 files changed, 9 insertions(+), 6 deletions(-)

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 111111111..111111111 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -562,15 +562,18 @@ enum zone_watermarks {
 };
 
 /*
- * One per migratetype for each PAGE_ALLOC_COSTLY_ORDER plus one additional
- * for pageblock size for THP if configured.
+ * One per migratetype for each PAGE_ALLOC_COSTLY_ORDER. One additional list
+ * for THP which will usually be GFP_MOVABLE. Even if it is another type,
+ * it should not contribute to serious fragmentation causing THP allocation
+ * failures.
  */
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 #define NR_PCP_THP 1
 #else
 #define NR_PCP_THP 0
 #endif
-#define NR_PCP_LISTS (MIGRATE_PCPTYPES * (PAGE_ALLOC_COSTLY_ORDER + 1 + NR_PCP_THP))
+#define NR_LOWORDER_PCP_LISTS (MIGRATE_PCPTYPES * (PAGE_ALLOC_COSTLY_ORDER + 1))
+#define NR_PCP_LISTS (NR_LOWORDER_PCP_LISTS + NR_PCP_THP)
 
 /*
  * Shift to encode migratetype and order in the same integer, with order
@@ -596,7 +599,7 @@ struct per_cpu_pages {
 
 	/* Lists of pages, one per migrate type stored on the pcp-lists */
 	struct list_head lists[NR_PCP_LISTS];
-};
+} ____cacheline_aligned_in_smp;
 
 struct per_cpu_zonestat {
 #ifdef CONFIG_SMP
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 111111111..111111111 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -655,7 +655,7 @@ static inline unsigned int order_to_pindex(int migratetype, int order)
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 	if (order > PAGE_ALLOC_COSTLY_ORDER) {
 		VM_BUG_ON(order != pageblock_order);
-		base = PAGE_ALLOC_COSTLY_ORDER + 1;
+		return NR_LOWORDER_PCP_LISTS;
 	}
 #else
 	VM_BUG_ON(order > PAGE_ALLOC_COSTLY_ORDER);
@@ -669,7 +669,7 @@ static inline int pindex_to_order(unsigned int pindex)
 	int order = pindex / MIGRATE_PCPTYPES;
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
-	if (order > PAGE_ALLOC_COSTLY_ORDER)
+	if (pindex == NR_LOWORDER_PCP_LISTS)
 		order = pageblock_order;
 #else
 	VM_BUG_ON(order > PAGE_ALLOC_COSTLY_ORDER);

